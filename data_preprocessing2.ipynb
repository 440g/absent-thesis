{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7d9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd297fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_FILE_PATH = './dataset/arxiv-metadata-oai-snapshot.json'\n",
    "    OUTPUT_EMBEDDING_PATH = './dataset/arxiv_paper_embeddings.pkl'\n",
    "    OUTPUT_PROCESSED_DATA_PATH = './dataset/processed_arxiv_data.pkl'\n",
    "    MODEL_NAME = 'all-mpnet-base-v2'\n",
    "    SAMPLE_SIZE_PER_YEAR = 1000\n",
    "    TIME_PERIODS = [(1986, 1992),  ] \n",
    "    # 1993년부터 2025년까지 1년 단위로 추가\n",
    "    for year in range(1993, 2026):\n",
    "        TIME_PERIODS.append((year, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5ba9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_versions(versions):\n",
    "    \"\"\"versions 필드에서 실제 제출 날짜 추출\"\"\"\n",
    "    if isinstance(versions, list) and len(versions) > 0:\n",
    "        first_version = versions[0]\n",
    "        created_date = first_version.get('created', '')\n",
    "        match = re.search(r'(\\d{4})', created_date)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_year_fallback(arxiv_id):\n",
    "    \"\"\"ID 기반 연도 추출 (폴백용)\"\"\"\n",
    "    if isinstance(arxiv_id, str) and '.' in arxiv_id:\n",
    "        year_prefix = arxiv_id.split('.')[0][:2]\n",
    "        try:\n",
    "            year_int = int(year_prefix)\n",
    "            if 90 <= year_int <= 99:\n",
    "                return 1900 + year_int\n",
    "            elif 0 <= year_int <= 25:\n",
    "                return 2000 + year_int\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def load_and_sample_data(file_path: str, sample_size_per_year: int) -> pd.DataFrame:\n",
    "    print(f\"> Loading data from {file_path}...\")\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    print(f\"> Total initial records: {len(df)}\")\n",
    "    \n",
    "    print(\"> Extracting years from versions...\")\n",
    "    years = []\n",
    "    for version in tqdm(df['versions'], desc=\"Extracting years\"):\n",
    "        years.append(extract_year_from_versions(version))\n",
    "    \n",
    "    df['year'] = years\n",
    "    \n",
    "    if df['year'].isna().sum() > 0:\n",
    "        print(\"> Some years missing from versions, falling back to ID-based extraction...\")\n",
    "        mask = df['year'].isna()\n",
    "        \n",
    "        fallback_years = []\n",
    "        for arxiv_id in tqdm(df.loc[mask, 'id'], desc=\"Fallback year extraction\"):\n",
    "            fallback_years.append(extract_year_fallback(arxiv_id))\n",
    "        \n",
    "        df.loc[mask, 'year'] = fallback_years\n",
    "    \n",
    "    df.dropna(subset=['year'], inplace=True)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    \n",
    "    print(f\"> Records with valid year: {len(df)}\")\n",
    "    print(f\"> Year range: {df['year'].min()} - {df['year'].max()}\")\n",
    "    \n",
    "    sampled_dfs = []\n",
    "    for year, group in df.groupby('year'):\n",
    "        sample_size = min(sample_size_per_year, len(group))\n",
    "        sample_df = group.sample(n=sample_size, random_state=42)\n",
    "        sampled_dfs.append(sample_df)\n",
    "    \n",
    "    df_sampled = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "    \n",
    "    # 시기 정보 추가\n",
    "    def assign_time_period(year):\n",
    "        for start, end in Config.TIME_PERIODS:\n",
    "            if start <= year <= end:\n",
    "                return f\"{start}-{end}\"\n",
    "        return \"other\"\n",
    "    \n",
    "    df_sampled['time_period'] = df_sampled['year'].apply(assign_time_period)\n",
    "    \n",
    "    print(f\"> Total sampled records: {len(df_sampled)}\")\n",
    "    print(f\"> Time period distribution:\\n{df_sampled['time_period'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return df_sampled\n",
    "\n",
    "def enhanced_preprocess_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # LaTeX 명령어 제거\n",
    "    text = re.sub(r'\\\\(?:[a-zA-Z]+|[{}])', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def prepare_topic_labeling_data(df):\n",
    "    print(\"> Preparing topic labeling data with TF-IDF...\")\n",
    "    \n",
    "    # TF-IDF 벡터라이저 설정\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=150, \n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 3)\n",
    "    )\n",
    "    \n",
    "    # TF-IDF 행렬 생성\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    print(f\"> Extracted {len(feature_names)} features\")\n",
    "    \n",
    "    # 각 문서별 상위 키워드 추출 (tqdm 수정)\n",
    "    top_keywords = []\n",
    "    for i in tqdm(range(len(df)), desc=\"Extracting keywords\"):\n",
    "        tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
    "        top_indices = tfidf_scores.argsort()[-5:][::-1]  # 상위 5개 키워드\n",
    "        keywords = [feature_names[idx] for idx in top_indices if tfidf_scores[idx] > 0]\n",
    "        top_keywords.append(keywords)\n",
    "    \n",
    "    df['top_keywords'] = top_keywords\n",
    "    df['top_keywords_str'] = df['top_keywords'].apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    # TF-IDF 정보 저장\n",
    "    tfidf_info = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'feature_names': feature_names,\n",
    "        'matrix': tfidf_matrix\n",
    "    }\n",
    "    \n",
    "    return df, tfidf_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ef3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    if not os.path.exists(Config.DATA_FILE_PATH):\n",
    "        print(f\"> Error: Data file not found at {Config.DATA_FILE_PATH}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # 1. 데이터 로드 및 샘플링\n",
    "        df_sampled = load_and_sample_data(\n",
    "            Config.DATA_FILE_PATH, \n",
    "            Config.SAMPLE_SIZE_PER_YEAR\n",
    "        )\n",
    "        \n",
    "        # 2. 텍스트 전처리 \n",
    "        print(\">> Preprocessing text data...\")\n",
    "        df_sampled['title'] = df_sampled['title'].fillna('')\n",
    "        df_sampled['abstract'] = df_sampled['abstract'].fillna('')\n",
    "        \n",
    "        print(\">> Extracting primary category...\")\n",
    "        df_sampled['primary_category'] = df_sampled['categories'].apply(\n",
    "            lambda x: str(x).split()[0] if isinstance(x, str) and str(x).strip() else ''\n",
    "        )\n",
    "        combined_texts = []\n",
    "        \n",
    "        texts_to_process = (df_sampled['title'] + '. ' + df_sampled['abstract']).fillna('').tolist() # .fillna('') 추가\n",
    "        \n",
    "        for text in tqdm(texts_to_process, desc=\"Preprocessing texts\"):\n",
    "            processed_text = enhanced_preprocess_text(text)\n",
    "            combined_texts.append(processed_text)\n",
    "        \n",
    "        df_sampled['combined_text'] = combined_texts\n",
    "        \n",
    "        # 3. 토픽 라벨링 준비\n",
    "        df_sampled, tfidf_info = prepare_topic_labeling_data(df_sampled)\n",
    "        \n",
    "        # 4. 임베딩 생성\n",
    "        print(\">> Generating embeddings...\")\n",
    "        model = SentenceTransformer(Config.MODEL_NAME)\n",
    "        embeddings = model.encode(\n",
    "            df_sampled['combined_text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "        # tqdm을 이용한 수동 진행률 표시 필요(큰 문제는 아님)\n",
    "        \n",
    "        # 5. 결과 저장\n",
    "        pd.DataFrame(embeddings_np).to_pickle(Config.OUTPUT_EMBEDDING_PATH)\n",
    "        df_sampled.to_pickle(Config.OUTPUT_PROCESSED_DATA_PATH)\n",
    "        \n",
    "        print(f\"\\n--- Processing Complete ---\")\n",
    "        print(f\"> Embeddings shape: {embeddings_np.shape}\")\n",
    "        print(f\"> Sample data with keywords:\")\n",
    "        print(df_sampled[['title', 'year', 'top_keywords_str']].head(3))\n",
    "        \n",
    "        return df_sampled, embeddings_np\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"> Error during processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30737a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading data from ./dataset/arxiv-metadata-oai-snapshot.json...\n",
      "> Total initial records: 2860945\n",
      "> Extracting years from versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting years: 100%|██████████| 2860945/2860945 [00:05<00:00, 513396.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Records with valid year: 2860945\n",
      "> Year range: 1986 - 2025\n",
      "> Total sampled records: 34387\n",
      "> Time period distribution:\n",
      "time_period\n",
      "1986-1992    1387\n",
      "1993-1993    1000\n",
      "1994-1994    1000\n",
      "1995-1995    1000\n",
      "1996-1996    1000\n",
      "1997-1997    1000\n",
      "1998-1998    1000\n",
      "1999-1999    1000\n",
      "2000-2000    1000\n",
      "2001-2001    1000\n",
      "2002-2002    1000\n",
      "2003-2003    1000\n",
      "2004-2004    1000\n",
      "2005-2005    1000\n",
      "2006-2006    1000\n",
      "2007-2007    1000\n",
      "2008-2008    1000\n",
      "2009-2009    1000\n",
      "2010-2010    1000\n",
      "2011-2011    1000\n",
      "2012-2012    1000\n",
      "2013-2013    1000\n",
      "2014-2014    1000\n",
      "2015-2015    1000\n",
      "2016-2016    1000\n",
      "2017-2017    1000\n",
      "2018-2018    1000\n",
      "2019-2019    1000\n",
      "2020-2020    1000\n",
      "2021-2021    1000\n",
      "2022-2022    1000\n",
      "2023-2023    1000\n",
      "2024-2024    1000\n",
      "2025-2025    1000\n",
      "Name: count, dtype: int64\n",
      ">> Preprocessing text data...\n",
      ">> Extracting primary category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing texts: 100%|██████████| 34387/34387 [00:00<00:00, 49432.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Preparing topic labeling data with TF-IDF...\n",
      "> Extracted 150 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|██████████| 34387/34387 [00:00<00:00, 56108.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70129078337948d5a1574070dab5cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Complete ---\n",
      "> Embeddings shape: (34387, 768)\n",
      "> Sample data with keywords:\n",
      "                              title  year                   top_keywords_str\n",
      "0  Desperately Seeking Superstrings  1986          provide, analysis, theory\n",
      "1    Applied Conformal Field Theory  1988  free, theory, field, critical, 10\n",
      "2      Convex bodies with few faces  1989                     number, theory\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df, embeddings = data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656389c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
