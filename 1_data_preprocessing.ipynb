{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663a4b3b",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2065d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "class Config:\n",
    "    DATA_FILE_PATH = './dataset/arxiv-metadata-oai-snapshot.json'\n",
    "    OUTPUT_RAW_DATA_CACHE = './dataset/arxiv_raw_cache.pkl'\n",
    "    OUTPUT_PROCESSED_DATA_INTERIM = './dataset/processed_arxiv_interim.pkl'\n",
    "    OUTPUT_PROCESSED_DATA_PATH = './dataset/processed_arxiv.pkl'\n",
    "    OUTPUT_EMBEDDING_PATH = './dataset/arxiv_embeddings_fp16.pkl'\n",
    "    MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    BATCH_SIZE = 32\n",
    "    SAMPLES_PER_YEAR = 100\n",
    "    PROFILE_SAMPLE_SIZE = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99492e4",
   "metadata": {},
   "source": [
    "## 1.1. 날짜 및 슬롯 추출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84a4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_versions(versions):\n",
    "    if isinstance(versions, list) and len(versions) > 0:\n",
    "        first_version = versions[0]\n",
    "        created_date = first_version.get('created', '')\n",
    "        match = re.search(r'(\\d{4})', created_date) \n",
    "        if match:\n",
    "            try:\n",
    "                year = int(match.group(1))\n",
    "                if 1990 < year <= 2025: # 유효한 연도 범위로 제한\n",
    "                    return year\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3eea7",
   "metadata": {},
   "source": [
    "## 1.2. 5년 단위 time slot 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e87cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_5yr_slot(y):\n",
    "    if y is None or y < 1991:\n",
    "        return None\n",
    "    start = ((y - 1) // 5) * 5 + 1 \n",
    "    end = start + 4\n",
    "    return f\"{start}-{end}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162cd0a",
   "metadata": {},
   "source": [
    "## 1.3. 데이터 로드 및 원본 캐시 생성 (Raw Data Load & Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29aa9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    # 1. 원본 캐시 파일 확인 및 로드\n",
    "    if Path(Config.OUTPUT_RAW_DATA_CACHE).exists():\n",
    "        print(f\"> Raw data cache found. Loading from: {Config.OUTPUT_RAW_DATA_CACHE}\")\n",
    "        with open(Config.OUTPUT_RAW_DATA_CACHE, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "        print(f\"> Successfully loaded {len(df)} raw records.\")\n",
    "        return df\n",
    "\n",
    "    # 2. 원본 JSON 파일 로딩\n",
    "    path = Config.DATA_FILE_PATH\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"> Not found: {path}.\")\n",
    "\n",
    "    rows = []\n",
    "    print(f\"> Raw data cache not found. Loading raw JSON from {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading JSON\"):\n",
    "            rows.append(json.loads(line))\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # 3. 로드된 원본 DataFrame 캐시 저장\n",
    "    Path(os.path.dirname(Config.OUTPUT_RAW_DATA_CACHE)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Config.OUTPUT_RAW_DATA_CACHE, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    \n",
    "    print(f\"\\n> Raw data loaded and cached. Total records: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc2b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Raw data cache found. Loading from: ./dataset/arxiv_raw_cache.pkl\n",
      "> Successfully loaded 2860945 raw records.\n"
     ]
    }
   ],
   "source": [
    "df_raw = load_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d1755",
   "metadata": {},
   "source": [
    "## 1.4. 전처리, 샘플링 및 최종 캐시 생성 (Preprocessing, Sampling & Final Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04eb0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"연도 추출, 필터링, 텍스트 병합을 수행하고 중간 결과를 저장합니다.\"\"\"\n",
    "    \n",
    "    # 1. 중간 캐시 파일 확인 및 로드 (Cache Check)\n",
    "    if Path(Config.OUTPUT_PROCESSED_DATA_INTERIM).exists():\n",
    "        print(f\"> Intermediate processed data found. Loading from: {Config.OUTPUT_PROCESSED_DATA_INTERIM}\")\n",
    "        with open(Config.OUTPUT_PROCESSED_DATA_INTERIM, 'rb') as f:\n",
    "            df_processed = pickle.load(f)\n",
    "        print(f\"> Successfully loaded {len(df_processed)} pre-processed records. Skipping preprocessing.\")\n",
    "        return df_processed\n",
    "        \n",
    "    print(\"> Intermediate cache not found. Starting preprocessing...\")\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    # 2. 연도 추출 \n",
    "    df_temp['year'] = df_temp['versions'].apply(extract_year_from_versions)\n",
    "    df_temp['year'] = df_temp['year'].astype('Int64', errors='ignore')\n",
    "\n",
    "    # 3. 필터링 및 텍스트 병합\n",
    "    df_processed = df_temp.dropna(subset=['title', 'abstract', 'year']).reset_index(drop=True)\n",
    "    df_processed['text'] = df_processed['title'].astype(str) + \" \" + df_processed['abstract'].astype(str)\n",
    "    df_processed = df_processed[df_processed['text'].str.strip().str.len() > 10].reset_index(drop=True) \n",
    "    print(f\"> After filtering (NaN, short text): {len(df_processed)} records.\")\n",
    "    \n",
    "    # 4. 중간 결과 캐시 저장\n",
    "    Path(os.path.dirname(Config.OUTPUT_PROCESSED_DATA_INTERIM)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Config.OUTPUT_PROCESSED_DATA_INTERIM, 'wb') as f:\n",
    "        pickle.dump(df_processed, f)\n",
    "        \n",
    "    print(f\"\\n> Preprocessing Done. Intermediate Total Records: {len(df_processed)}\")\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431be7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Intermediate processed data found. Loading from: ./dataset/processed_arxiv_interim.pkl\n",
      "> Successfully loaded 2860911 pre-processed records. Skipping preprocessing.\n"
     ]
    }
   ],
   "source": [
    "df_processed = preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775eaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. 샘플링 및 최종 캐시 생성 (Sampling & Final Cache)\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_sampling_and_final_slot(df_processed):\n",
    "    \"\"\"전처리된 DataFrame을 연간 샘플링하고 최종 Time Slot을 생성합니다.\"\"\"\n",
    "    \n",
    "    # 1. 최종 캐시 파일 확인 및 로드 (Cache Check)\n",
    "    if Path(Config.OUTPUT_PROCESSED_DATA_PATH).exists():\n",
    "        print(f\"> Final sampled data found. Loading from: {Config.OUTPUT_PROCESSED_DATA_PATH}\")\n",
    "        with open(Config.OUTPUT_PROCESSED_DATA_PATH, 'rb') as f:\n",
    "            df_final = pickle.load(f)\n",
    "        print(f\"> Successfully loaded {len(df_final)} final sampled records. Skipping sampling.\")\n",
    "        return df_final\n",
    "        \n",
    "    print(\"> Sampled cache not found. Starting annual stratified sampling...\")\n",
    "\n",
    "    # 2. 연도별 계층적 샘플링 (Stratified Sampling)\n",
    "    df_sampling_base = df_processed.copy()\n",
    "    if len(df_sampling_base) > 0:\n",
    "        sample_size = Config.SAMPLES_PER_YEAR\n",
    "        def safe_sample(group):\n",
    "            if len(group) < sample_size:\n",
    "                return group\n",
    "            return group.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "        df_sampled = df_sampling_base.groupby('year', group_keys=False).apply(safe_sample).reset_index(drop=True)\n",
    "        print(f\"> Stratified Sampling (by Year) Applied. Total Samples: {len(df_sampled)}\")\n",
    "        df_final = df_sampled\n",
    "    else:\n",
    "        df_final = df_processed\n",
    "        \n",
    "    # 3. Time Slot 생성 및 최종 필터링\n",
    "    df_final['slot'] = df_final['year'].apply(to_5yr_slot)\n",
    "    df_final = df_final.dropna(subset=['slot']).reset_index(drop=True) \n",
    "\n",
    "    # 4. 최종 결과 캐시 저장\n",
    "    Path(os.path.dirname(Config.OUTPUT_PROCESSED_DATA_PATH)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Config.OUTPUT_PROCESSED_DATA_PATH, 'wb') as f:\n",
    "        pickle.dump(df_final, f)\n",
    "        \n",
    "    print(f\"\\n> Sampling Done. Final Total Records: {len(df_final)}\")\n",
    "    print(f\"--- Final Slot Distribution ---\\n{df_final['slot'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f720c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Sampled cache not found. Starting annual stratified sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/v_32vyl14g5dm3wm3452sjrc0000gn/T/ipykernel_31138/2928811203.py:27: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_sampling_base.groupby('year', group_keys=False).apply(safe_sample).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Stratified Sampling (by Year) Applied. Total Samples: 3500\n",
      "\n",
      "> Sampling Done. Final Total Records: 3500\n",
      "--- Final Slot Distribution ---\n",
      "slot\n",
      "1991-1995    500\n",
      "1996-2000    500\n",
      "2001-2005    500\n",
      "2006-2010    500\n",
      "2011-2015    500\n",
      "2016-2020    500\n",
      "2021-2025    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = apply_sampling_and_final_slot(df_processed)\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26018d",
   "metadata": {},
   "source": [
    "## 1.5. SBERT 모델 로드 및 인퍼런스 프로파일링 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eaddbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sbert(model_option='FP32'):\n",
    "    \"\"\"최적화 옵션에 따라 SBERT 모델을 로드합니다.\"\"\"\n",
    "    model = SentenceTransformer(Config.MODEL_NAME)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    elif torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    model.to(device)\n",
    "\n",
    "    if model_option == 'FP16' and device in (\"cuda\", \"mps\"):\n",
    "        try:\n",
    "            model.half()\n",
    "        except:\n",
    "            print(f\"> Warning: FP16 failed on {device}. Falling back to FP32.\")\n",
    "            model_option = 'FP32'\n",
    "    \n",
    "    if model_option == 'COMPILE':\n",
    "        if hasattr(torch, 'compile'):\n",
    "            model.encode = torch.compile(model.encode, mode=\"reduce-overhead\")\n",
    "            print(\"> torch.compile enabled (if available).\")\n",
    "        else:\n",
    "            model_option = 'FP32'\n",
    "\n",
    "    model.eval()\n",
    "    return model, device, model_option\n",
    "\n",
    "\n",
    "def profile_and_embed(texts, batch_size=Config.BATCH_SIZE):\n",
    "    \"\"\"최적화 옵션별 성능을 프로파일링하고 결과를 반환합니다.\"\"\"\n",
    "    \n",
    "    profile_results = []\n",
    "    options = ['FP32', 'FP16']\n",
    "    profile_texts = texts[:Config.PROFILE_SAMPLE_SIZE]\n",
    "    \n",
    "    print(\"\\n--- Inference Profiling Start (Optimization Demo) ---\")\n",
    "    for opt in options:\n",
    "        try:\n",
    "            # 1. 모델 로드 및 워밍업\n",
    "            model, device, final_opt = load_sbert(model_option=opt)\n",
    "            model.encode(profile_texts[:batch_size], batch_size=batch_size, device=device)\n",
    "            \n",
    "            # 2. 실제 프로파일링\n",
    "            start_time = time.time()\n",
    "            for i in range(0, len(profile_texts), batch_size):\n",
    "                batch = profile_texts[i:i+batch_size]\n",
    "                with torch.no_grad():\n",
    "                    _ = model.encode(batch, batch_size=batch_size, device=device)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            duration = end_time - start_time\n",
    "            throughput = len(profile_texts) / duration \n",
    "            mem = 0\n",
    "            if device == 'cuda':\n",
    "                mem = torch.cuda.max_memory_allocated(device=device) / (1024**3)\n",
    "            elif device == 'mps':\n",
    "                try:\n",
    "                    mem = torch.mps.current_allocated_memory() / (1024**3)\n",
    "                except AttributeError:\n",
    "                    print(f\"\\n[Warning] Accurate memory tracking for MPS failed. Reporting 0.0 GB.\")\n",
    "                    mem = 0.0      \n",
    "            profile_results.append({\n",
    "                'Option': final_opt,\n",
    "                'Throughput (s/s)': round(throughput, 2),\n",
    "                'Memory (GB)': round(mem, 2),\n",
    "                'Duration (s)': round(duration, 2)\n",
    "            })\n",
    "            \n",
    "            print(f\"[{final_opt}] Throughput: {throughput:.2f} s/s, Mem: {mem:.2f} GB\")\n",
    "            del model\n",
    "            if device in ('cuda', 'mps'):\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Profiling failed for [{opt}]: {e}\")\n",
    "            \n",
    "    profile_df = pd.DataFrame(profile_results)\n",
    "    print(\"\\n--- Profiling Result Table  ---\")\n",
    "    print(profile_df)\n",
    "    \n",
    "    return profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8be183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Profiling Start (Optimization Demo) ---\n",
      "[FP32] Throughput: 34.27 s/s, Mem: 1.90 GB\n",
      "[FP16] Throughput: 40.58 s/s, Mem: 2.69 GB\n",
      "\n",
      "--- Profiling Result Table  ---\n",
      "  Option  Throughput (s/s)  Memory (GB)  Duration (s)\n",
      "0   FP32             34.27         1.90         58.36\n",
      "1   FP16             40.58         2.69         49.29\n"
     ]
    }
   ],
   "source": [
    "profile_df = profile_and_embed(texts, batch_size=Config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a03199",
   "metadata": {},
   "source": [
    "## 1.6 전체 데이터셋 임베딩 생성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_embedding(texts, profile_df, batch_size=Config.BATCH_SIZE):\n",
    "    \"\"\"전체 데이터셋에 대해 임베딩을 생성하고 결과를 저장합니다.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- Final Embedding Generation (FP16 Selected for Stability) ---\")\n",
    "    \n",
    "    model_final, device_final, final_opt = load_sbert(model_option='FP16')\n",
    "    print(f\"> Final embedding using: {final_opt} on {device_final}\")\n",
    "    \n",
    "    final_embs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "            out = model_final.encode(\n",
    "                batch,\n",
    "                batch_size=batch_size,\n",
    "                convert_to_tensor=True,\n",
    "                device=device_final\n",
    "            )\n",
    "        final_embs.append(out.cpu())\n",
    "    \n",
    "    embeddings = torch.cat(final_embs, dim=0)\n",
    "\n",
    "    # 결과 캐시\n",
    "    Path(os.path.dirname(Config.OUTPUT_EMBEDDING_PATH)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Config.OUTPUT_EMBEDDING_PATH, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"ids\": df['id'].tolist(),\n",
    "            \"emb\": embeddings,\n",
    "            \"slot\": df['slot'].tolist(),\n",
    "            \"year\": df['year'].tolist(),\n",
    "            \"profile\": profile_df.to_dict('records') # 최적화 결과 저장\n",
    "        }, f)\n",
    "\n",
    "    print(f\"\\n> Done. Final Embeddings ({embeddings.shape}) and profiling results saved to {Config.OUTPUT_EMBEDDING_PATH}\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ddf0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Embedding Generation (FP16 Selected for Stability) ---\n",
      "> Final embedding using: FP16 on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 110/110 [01:30<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Done. Final Embeddings (torch.Size([3500, 768])) and profiling results saved to ./dataset/arxiv_embeddings_fp16.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = generate_final_embedding(texts, profile_df, batch_size=Config.BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
